{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Régression logistique\n",
    "\n",
    "La régression logistique est utilisée pour le classement et pas la régression.\n",
    "Mais, elle est considéré comme une méthode de régression puisqu'elle sert à estimer la probabilité d'appartenir à une classe.\n",
    "Il y a trois types de régression logistique:\n",
    "- **Régression logistique binaire**: ici, le but de la classification est d'identifier si un échantillon appartient à une classe ou non.\n",
    "- **Régression logistique multinomiale**: \n",
    ", le but de la classification est d'identifier à quelle classe appartient-t-il un échantillon parmi plusieurs classes.\n",
    "- **Régression logistique ordinale**: ici, le but de la classification est de chercher la classe d'un échantillon parmi des classes ordonnées. Un exemple de classes: non satisfait, satisfait, très sataisfait.\n",
    "\n",
    "### 1-  Principe\n",
    "\n",
    "Pour combiner entre les différentes caractéristiques, on utilise une fonction linéaire (exactement comme la régression linéaire):\n",
    "\n",
    "$$h_{w}(x) = w_0 + w_1 x_1 + w_2 x_2 + \\dots+ w_p x_p$$\n",
    "\n",
    "Cette valeur est transformée à une probabilité en utilisant la fonction logistique.\n",
    "Donc, la probabilité qu'un échantillon avec les caractéristiques *x_1, \\dots , x_p* appartienne à une classe *y_i* est calculée comme suit:\n",
    "\n",
    "$$ \\mathbb{p}(y=1|x)= \\frac{1}{1+\\exp(-h_w(x))}$$\n",
    "\n",
    "\n",
    "### 2- La décision\n",
    "\n",
    "Pour prédire si un échantillon *x* appartient à une classe donnée (classe positive) *y=1*, on calcule sa probabilité en utilisant l'équation précédante.\n",
    "Ensuite, on utilise un seuil sur cette probabilité pour décider.\n",
    "\n",
    "On peut utiliser le seuil **0.5**. Dans ce cas:\n",
    "- Si $\\mathbb{p}(y=1|x)\\ge 0.5$ donc classe positive\n",
    "- Sinon classe négative\n",
    "\n",
    "En cas de  plusieurs classes, on utilise une stratégie de un-contre-le-reste.\n",
    "On entraine plusieurs classifieurs, chacun pour une classe.\n",
    "Pour décider quelle est la classe d'un échantillon, on prend celle avec la probabilité la plus élevée.\n",
    "\n",
    "### 3- La fonction du coût\n",
    "\n",
    "L'erreur quadratique moyenne (MSE) ne peut pas être utilisée comme dans la régression linéaire.\n",
    "Ceci est dû au fait que la fonction de prédiction est non linéaire.\n",
    "La fonction du coût va être non-convex avec plusieurs minimums locaux.\n",
    "Lors de la minimisation, on peut tomber sur un minimum local et l'algorithme du gradient va s'arrêter sans converger vers la solution optimale.\n",
    "\n",
    "Dans ce cas, on utilise l'entropie croisée.\n",
    "Etant donnée un ensemble de données avec $n$ échantillons, où le résulat $y$ est soit $1$ ou $0$.\n",
    "La fonction du coût est calculée comme suit, où $(i)$ réfère au i-ème échantillon  dans les données d'entrainement:\n",
    "\n",
    "### 4- Descente de Gradient \n",
    "\n",
    "Puisque $y$ peut prendre seulement les deux valeurs $0$ et $1$, cette fonction peut être simplifiée comme suit:\n",
    "\n",
    "\n",
    "$$ w = w - \\alpha . dw $$\n",
    "$$ b = b - \\alpha . dw $$\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ \\frac{\\partial J(\\omega)}{\\partial \\omega} = \\frac{1}{n}X^T(h_{\\omega}(x)-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rappel sur la descente du gradient \n",
    "$f(x,y) = x^2 +  y^2$\n",
    "\n",
    "$\\arg \\min_{x,y} f(x,y)$\n",
    "\n",
    "$ x^{(i+1)} = x^{(i)} - \\alpha \\times \\nabla f( x^{(i)})$\n",
    "\n",
    "$ y^{(i+1)} = y^{(i)} - \\alpha \\times \\nabla f( y^{(i)})$\n",
    "\n",
    "-----\n",
    "\n",
    "Resolution \n",
    "\n",
    "$\\nabla f(x, y)= (2x, 2y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation sous une version simple \n",
    "x , y = 1,  1\n",
    "alpha = 0.02 \n",
    "\n",
    "for i  in range(1,10): \n",
    "    x = x -   alpha * 2*x \n",
    "    print(x)\n",
    "    y = y  - alpha * 2*y \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation avec numpy \n",
    "x = np.array([1,1])\n",
    "\n",
    "def gradient(x): \n",
    "    return 2 *x \n",
    "\n",
    "for i in range(100):\n",
    "    x = x - alpha * gradient(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UTILISATION DE SKLEARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import seaborn as sns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# premier jeu de donnée \n",
    "# X, y = make_classification(n_samples=1000, n_features=10, random_state=10)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "# secod jeu de donée \n",
    "\n",
    "X = load_iris().data\n",
    "y = load_iris().target \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPLEMENTATION FROM SCRATCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression: \n",
    "    def __init__(self, n_iters=100, lr=0.01): \n",
    "        self.n_iters = n_iters \n",
    "        self.weight = None \n",
    "        self.lr = lr \n",
    "    \n",
    "    def fit(self, X, y): \n",
    "        n_samples, n_features = X.shape \n",
    "        \n",
    "        X = np.concatenate([np.ones(n_samples).reshape(-1,1), X], axis = 1)\n",
    "\n",
    "        # Initialisation des parametres \n",
    "        self.weight = np.random.random(n_features+1)\n",
    "        \n",
    "\n",
    "        # optimisation des parametres avec la descente du gradient \n",
    "\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            predicted = X.dot(self.weight)\n",
    "            gradient = (1/n_samples)*(np.transpose(X).dot(predicted-y))\n",
    "            #update des poids \n",
    "            self.weight = self.weight - self.lr* gradient\n",
    "\n",
    "    def _sigmoid(self,x):\n",
    "        \n",
    "        return  1/ (1 + math.exp(-x)) \n",
    "\n",
    "    \n",
    "\n",
    "    def predict_poba(self, x):\n",
    "        x = np.concatenate([[1],x])\n",
    "        h_w = x.dot(self.weight)\n",
    "        predict_prob = self._sigmoid(h_w)\n",
    "        return predict_prob\n",
    "    \n",
    "    def _predict(self,x):\n",
    "        prob = self.predict_poba(x)\n",
    "        return 1 if prob>= 0.5 else 0\n",
    "\n",
    "    def predict(self, X): \n",
    "        pred = [self._predict(x) for x in X ]\n",
    "        return np.array(pred)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisation du model implementé from scratch \n",
    "model = LogisticRegression(n_iters=10000)\n",
    "model.fit(X, y)\n",
    "prediction = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5466666666666666"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prediction == y).sum() / X.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y )\n",
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred== y).sum() / X.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae728774eee256562aff2651c76309c3916f29437b55f618701e1f4834ebef1b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
